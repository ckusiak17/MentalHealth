---
title: "CDA Final Project"
author: "Joshua Freeman, Coco Kusiak, and Luke Toomey"
date: "12/12/2017"
toc: TRUE
toc_depth: 2
number_sections: TRUE
bibliography: bibliography.bib
output: pdf_document
---

\newpage

#Introduction    

##Motivation  

Depression and anxiety disorders are the most common mental illnesses in the United States [@ada]. Without proper treatment, these conditions can become chronic diseases and lead to increased risk for mortality [@cdc]. Although many treatments are available for these conditions unfortunately, only about 37% of those with anxiety seek treatment [-@ada]. Having just one depressive episode leaves the afflicted person with a 50% of experiencing another [-@cdc]. We set out to see what is predictive of having poor mental health in the average American.  



##The Data  

```{r cleaning_data, echo=FALSE, message = FALSE}
require(mosaic)
load("mhfp2.Rda")
mhfp<-bstat743fp
mhfp <- plyr :: rename(mhfp, c("SLEPTIM1"="sleep", "MENTHLTH"="mental", "SEX" = "sex", "EMPLOY1"="employ", "SXORIENT"="sexorient",
                       "TRNSGNDR"="trans", "QLACTLM2"="actlimit", "MSCODE"="metro", "_TOTINDA"="exer30", 
                       "_RACE_G1"="race", "_RFBMI5"="bmi",  "_INCOMG"="income", "_SMOKER3"="smoker",
                       "_RFBING5"="binge", "_LLCPWT"="weight"))
mhfp <- mutate(mhfp, male = ifelse(sex == 1, yes = 1, no = ifelse(sex == 2, yes = 0, no = NA)),
               mental = ifelse(mental == 88, yes = 0, no = ifelse(mental %in% c(77, 99), yes = NA, no = 1)),
               sleep = ifelse(sleep == 99 | sleep == 77, yes = NA, no = sleep),
               sexorient = ifelse(sexorient == 1, yes = "straight", 
                                  no = ifelse(sexorient == 2, yes = "gay", 
                                              no = ifelse(sexorient == 3, "bisexual", 
                                                          no = ifelse(sexorient == 4, yes = "other", no = NA)))),
               exer30 = ifelse(exer30 == 1, yes = 1, 
                               no = ifelse(exer30 == 2, yes = 0, no =NA)),
               trans = ifelse(trans %in% c(1, 2, 3), yes = 1, 
                              no = ifelse(trans == 4, yes = 0, no = NA)),
               smoker = ifelse(smoker %in% c(1, 2), yes = 1, 
                               no = ifelse(smoker == 3, yes = 2, 
                                           no = ifelse(smoker == 4, yes = 0, no =NA))),
               income = ifelse(income == 9, yes = NA, no = income),
               actlimit = ifelse(actlimit %in% c(7, 9 ), yes = NA, no = actlimit),
               employ = ifelse(employ == 9, yes = NA, no = employ),
               bmi = ifelse(bmi == 1, yes = 0, no = ifelse(bmi == 2, yes = 1, no = NA)),
               binge = ifelse(binge == 1, yes = 0, no = ifelse(binge == 2, yes = 1, no =NA))
)
mhfp<-subset(mhfp, select=-sex)

mhfp$income <-  as.factor(mhfp$income)
mhfp$metro <-  as.factor(mhfp$metro)
mhfp$smoker <-  as.factor(mhfp$smoker)
mhfp$employ <-  as.factor(mhfp$employ)
mhfp$trans <-  as.factor(mhfp$trans)
mhfp$actlimit <-  as.factor(mhfp$actlimit)
mhfp$sexorient <- as.factor(mhfp$sexorient)
mhfp$race <- as.factor(mhfp$race)

#save(mhfp, file = "mental_final.Rda")
```


##Results

\newpage

#Missing Data  

\pagebreak

#Automatic Model Selection Methods  

##Introduction  

The goal of my project is to test the performance of the different model selection algorithms ridge, LASSO, and stepwise regressions. LASSO and Ridge regressions are both forms of coefficient regularization. In addition to minimizing the residual sums of squares, these methods penalize covariates in our model based on some constraints. For ridge regression this can be written as: $\sum_{j=1}^p \beta^2_j < c$ with p = # of predictors and c a constant. For LASSO, this can be written as $\sum_{j=1}^p |\beta_j| < c$ [@stanford]. The important distinction between these two methods is that the LASSO method acutally drops predictors out of the model while the ridge method only shrinks coefficients close to 0.   

Stepwise regression adds or removes variables based on which improves the model's AIC. Forward selection begins with no predictors and tests which one additional variable will improve the model. This continues until AIC improvements stop. Backwards selection begins with all available predictors and removes them one-by-one until AIC improvements stop [@ncss]. For the purpose of this study, we will focus on a combination of both forward and backward selection.     

```{r, include=FALSE, message=FALSE}
require(glmnet)
require(MASS)
require(pROC)
require(mosaic)
require(xtable)
options(xtable.comment = FALSE)
```


##Step 1: Performance Assessment Based on Observed Data  

To begin, I fit these three algorithms on the full data.   

```{r, echo=FALSE, fig.height=4}
load("coco.Rda")

par(mfrow = c(1, 2))
plot(ridge, xvar = "dev", xlab = "Ridge Regression")
plot(lasso, xvar = "dev", xlab = "LASSO Regression")
title("Coefficient Shrinkage vs. Explained Deviance", outer=TRUE)
```   

The plot above show the coefficient shrinkage as a function of the model's fraction of explained deviance. Each curve represents a predictor. At the far right, all predictors are unpenalized. As you move to the left, the explained deviance decreases as the coefficients shrink closer to 0. The numbers along the upper x-axis represent the number of predictors remaining in the model. The Ridge model maintains all 14 variables even when the explained deviance is essentially zero. At this point in the LASSO plot, 0 predictors remain. The Ridge method shrinks coefficients very _close_ to 0, but the LASSO method drops some coefficients exactly to 0, removing them completely from the model. 

```{r, echo=FALSE, results ='asis'}
step1 <- filter(coefficients, variable %in% c("binge", "bmi", "exer30", "male", "sleep"))
print(xtable(step1), caption = "Subset of Coefficient Estimates from Selection Algorithms",
      caption.placement = "top")
```  
As shown on the previous page, most of the coefficient estimates are similar between the algorithms. However, for `bmi` stepwise and ridge estimate oppisite directions of association with the probability of having poor mental health, while the LASSO method drops it out of the model completely.  

Next, I run 10-fold cross validation for each of these selection and the results are shown below. 

```{r, echo = FALSE, results='asis'}
compare <- data.frame(Mean.AUC = round(c(mean(auc.test.aic), mean(auc.test.lasso), 
                                         mean(auc.test.ridge)), 3), 
                      SD.AUC = c(round(sd(auc.test.aic), 5), round(sd(auc.test.lasso), 5),
                                 round(sd(auc.test.ridge),5)))
rownames(compare) <- c("Stepwise", "LASSO", "Ridge")
print(xtable(compare), caption = "Mean Areas Under The Curve from Cross Validation",
      caption.placement = 'top')
```  
As can be seen above, the stepwise method yields the highest area under the curve across the 10 iterations. This model will be used in subsequent analyses.  

##Step 2: Simulation Study    

The next step in this investigation is running a simulation study [@sim].



##Conclusions  






\newpage

#Random Effect Models 



# References
