---
title: "CDA Final Project"
author: "Joshua Freeman, Coco Kusiak, and Luke Toomey"
date: "12/12/2017"
output:
  word_document: default
  pdf_document: default
number_sections: yes
bibliography: bibliography.bib
toc: yes
toc_depth: 2
---

\newpage

#Introduction    

##Motivation  

Depression and anxiety disorders are the most common mental illnesses in the United States [@ada]. Without proper treatment, these conditions can become chronic diseases and lead to increased risk for mortality [@cdc]. Although many treatments are available for these conditions unfortunately, only about 37% of those with anxiety seek treatment [-@ada]. Having just one depressive episode leaves the afflicted person with a 50% of experiencing another [-@cdc]. We set out to see what is predictive of having poor mental health in the average American.  

##The Data  

```{r cleaning_data, echo=FALSE, message = FALSE}
require(mosaic)
require(xtable)
require(dplyr)
load("mhfp2.Rda")
mhfp<-bstat743fp
mhfp<-mhfp[,-c(6)]
mhfp <- plyr :: rename(mhfp, c("SLEPTIM1"="sleep", "MENTHLTH"="mental", "SEX" = "sex", "EMPLOY1"="employ", "SXORIENT"="sexorient",
                       "TRNSGNDR"="trans", "QLACTLM2"="actlimit", "MSCODE"="metro", "_TOTINDA"="exer30", 
                       "_RACE_G1"="race", "_RFBMI5"="bmi",  "_INCOMG"="income", "_SMOKER3"="smoker",
                       "_RFBING5"="binge", "_LLCPWT"="weight", "_STATE"="state"))
mhfp <- mutate(mhfp, male = ifelse(sex == 1, yes = 1, no = ifelse(sex == 2, yes = 0, no = NA)),
               mental = ifelse(mental == 88, yes = 0, no = ifelse(mental %in% c(77, 99), yes = NA, no = 1)),
               sleep = ifelse(sleep == 99 | sleep == 77, yes = NA, no = sleep),
               sexorient = ifelse(sexorient == 1, yes = "straight", 
                                  no = ifelse(sexorient == 2, yes = "gay", 
                                              no = ifelse(sexorient == 3, "bisexual", 
                                                          no = ifelse(sexorient == 4, yes = "other", no = NA)))),
               exer30 = ifelse(exer30 == 1, yes = 1, 
                               no = ifelse(exer30 == 2, yes = 0, no =NA)),
               trans = ifelse(trans %in% c(1, 2, 3), yes = 1, 
                              no = ifelse(trans == 4, yes = 0, no = NA)),
               smoker = ifelse(smoker %in% c(1, 2), yes = 1, 
                               no = ifelse(smoker == 3, yes = 2, 
                                           no = ifelse(smoker == 4, yes = 0, no =NA))),
               income = ifelse(income == 9, yes = NA, no = income),
               actlimit = ifelse(actlimit %in% c(7, 9 ), yes = NA, no = actlimit),
               employ = ifelse(employ == 9, yes = NA, no = employ),
               bmi = ifelse(bmi == 1, yes = 0, no = ifelse(bmi == 2, yes = 1, no = NA)),
               binge = ifelse(binge == 1, yes = 0, no = ifelse(binge == 2, yes = 1, no =NA))
)
mhfp<-subset(mhfp, select=-sex)

mhfp$income <-  as.factor(mhfp$income)
mhfp$metro <-  as.factor(mhfp$metro)
mhfp$smoker <-  as.factor(mhfp$smoker)
mhfp$employ <-  as.factor(mhfp$employ)
mhfp$trans <-  as.factor(mhfp$trans)
mhfp$actlimit <-  as.factor(mhfp$actlimit)
mhfp$sexorient <- as.factor(mhfp$sexorient)
mhfp$race <- as.factor(mhfp$race)

#save(mhfp, file = "mental_final.Rda")

xtable(summary(mhfp))

table.income<-table(mhfp$income)
table.metro<-table(mhfp$metro)
table.smoker<-table(mhfp$smoker)
table.employ<-table(mhfp$employ)
table.trans<-table(mhfp$trans)
table.actlimit<-table(mhfp$actlimit)
table.sexor<-table(mhfp$sexorient)

table.income.perc = 100*prop.table(table.income)
table.metro.perc = 100*prop.table(table.metro)
table.smoker.perc = 100*prop.table(table.smoker)
table.employ.perc = 100*prop.table(table.employ)
table.trans.perc = 100*prop.table(table.trans)
table.actlimit.perc = 100*prop.table(table.actlimit)
table.sexor.perc = 100*prop.table(table.sexor)
```


##Brief Take-aways

- talk about important variables and their direction of association


\newpage

#Missing Data

Missing data are a common nuisance in data analysis.  The reasons for missing data in a dataset are myriad and can stem from any of the following reasons:  1.  Subjects miss a given visit due to administrative reasons or impracticality in visiting, 2. Failure of equipment or personnel in data collection procedures, 3.  Loss of interest in the study, 4.  Mortality due to competing factors, 5. Treatment effects, 6.  Unobserved health outcomes, 7. Survey design.  However the specific reason for missing data entails different consequences for the data and analysis quality.  Missingness will always impact the sample size and power of a statistical analysis to detect an effect.  However, if missingness is due to an underlying systematic error or inherent selection bias, then the results will be biased.  
Three models of missingness describe the general patterns missing data mechanisms.  Missing completely at random (MCAR) is when the probability of missingness in a given variable is independent of any other variable that is observed.  Missing at random (MAR) is a general assumption that the probability of variable missingness is dependent only on variables that were observed.  Missing not at random (MNAR) is the probability that missingness depends on unobserved (missing) data.  It is difficult to determine which missing data mechanism occurred in a dataset.  By definition, MAR cannot be assessed given the MNAR assumption relies on unobserved data.  MCAR can be assessed in certain situations.  If missing data differs between groups with respect to a certain outcome or at baseline, then certain assumptions can be made.  However, most missing data mechanisms are rarely MCAR and it can be assumed they are either MAR or MNAR.
Many methods have been developed in order to account for missingness in data and have advantages and disadvantages depending on the missing data mechanism.  Complete Case (CC) analysis is analysis based on using only observed data.  In other words missing is completely dropped from the dataset.  Under MCAR assumptions, CC is unbiased.  However, if data are truly MAR or MNAR, biases may result.  Additionally, depending on the percentage missing, substantial power for statistical inferences can be lost.  Hence, the general rule of thumb is to only use CC when missingness is less than 5% for each variable.  Multiple Imputation (MI) is another technique to reduce the effects of missingness.  Under MAR assumptions, MI attempts to retain maximum likelihood (ML)-based estimation while also allowing for variation due to the imputation method.  This is accomplished through using missing value replacements determined by chained equations based on repeatedly sampled observed data points.  These are entered into datasets that are averaged to obtain the imputed dataset.  This imputation method approximates the observed data likelihood with the missing values that were imputed.  Thus, the ML-based estimation is conserved while allowing for additional variance across multiple completed datasets.  In comparing the two methods, CC often yields biased estimates while MI has been shown to yield unbiased estimates up to 30% missingness.  However, this depends on the data, the size of the data and whether or not it is MCAR, MAR, or MNAR.
In assessing predictors of mental health state in the past 30 days, we propose to assess the performance of different analyses models using CC and MI using 20 repeated samples for 10 datasets, and MI using 50 repeated samples for 10 datasets.  We will compare odds ratios and standard errors generated by each missing procedure in order to assess CC and MI performance given differential levels of missing data assuming a MAR mechanism.

#Methods:

*Study population:*

	We used data based on the 2016 wave of the Behavioral Risk Factor Surveillance System (BRFSS).  The BRFSS is a nationally representative cross-sectional study assessing a battery of risk factors and disease outcomes.  Participants (n=486303) were recruited via landline or cell-phone based random digit dialing (RDD).  Participants must have been aged 18 and older, and had access to a land-based or cellular phone.

*Multiple Imputation:*

	We used the Multivariate Imputation by Chained Equations (MICE) package designed by van Buuren S. (2017).  This missing data method employs multiple imputations for each missing variable based on Fully Conditional Specification (FCS).  FCS uses a separate model to impute each missing variable.  The base algorithm specifies n=5 datasets and n=5 maximum iterations.  We elected to use n=10 datasets and n=20 (MI20) and n=50 (MI50) maximum iterations.  Graham et al. (2007) posits that in order to preserve maximum power using multiple imputation, 20 iterations should be used for between 10% and 30% missing data and 40 iterations for up to 50% missing data.  However, Bodner et al. (2008) and Royston et al. (2011) proposed the number of imputations should be based in the percentage of missing data when using Markov Chain Monte Carlo methods.  Hence, given our maximum percentage missing data was approximately 59% and given technical limitations in CPU power we used 50 iterations.  For the method specification, we used method=”sample” which uses a random sampling method among observed data to arrive at complete datasets.

*Statistical Analyses:*

We used binomial logistic regression models in the glm package in R to derive odds ratio estimates and standard errors for each variable in our final model and compared them across different missing data methods.  As noted in the methods by Coco Kusiak (CK), we designed three models for automatic variable selection of the most parsimonious model.  These models were compared using AIC.  The stepwise-selected model was chose as the most parsimonious, which is represented below:

*Results and Data Generation:*  

Overall there was missing that was differential across variables ranging from 0% missingness to 58.91% missingness (state and sexual orientation respectively) (Table 1.).  In comparing the CC to MI analyses, there were stark contrasts in beta coefficient magnitude and direction (Table 2.).  The beta coefficients for income changed direction from a negative to positive to negative coefficient going from CC to MI20 and MI50 analyses.  Almost all beta coefficients decreased in magnitude toward the null from the CC to MI analyses except for sleep, ever smoker, binge drinker statuses, and exercise in the last 30 days.  Beta coefficients were mostly similar between the MI20 and MI50 analyses; however, standard errors for the MI50 analyses were much smaller than the CC or MI20 analyses.

Table 1.Percent Missing by Variable
```{r}
library(xtable)
x<-as.data.frame(colMeans(is.na(mhfp)))
names(x)[names(x)=="colMeans(is.na(mhfp))"]<-"Percent Missing"
xtable(x)
```

#Multiple Imputations MICE##
```{r}
#library(mice)
#library(VIM)
#md.pattern(mhfp)

# ##20 iterations##
# imputedat<-mice(mhfp, m=10, maxit = 20, method = 'sample', seed = 500, print=FALSE)
# summary(imputedat)
# compdat<-complete(imputedat,2)
# save(imputedat, file = "imputedsets.Rda")
# save(compdat, file = "completeimputed.Rda")
# 
# ##50 iterations##
# imputedat50<-mice(mhfp, m=10, maxit = 50, method = 'sample', seed = 500, print=FALSE)
# summary(imputedat50)
# compdat50<-complete(imputedat50,2)
# save(imputedat50, file = "imputedsets50.Rda")
# save(compdat50, file="completeimputed50.Rda")
```


```{r, echo=FALSE, results='asis'}  
library(mice)
library(VIM)
library(xtable)
load("imputedsets.Rda")
load("completeimputed.Rda")
# compdat$income<-as.factor(compdat$income)
# compdat$state<-as.factor(compdat$state)
# compdat$metro<-as.factor(compdat$metro)
# compdat$smoker<-as.factor(compdat$smoker)
# compdat$employ<-as.factor(compdat$employ)
# compdat$trans<-as.factor(compdat$trans)
# compdat$actlimit<-as.factor(compdat$actlimit)
# 
# mhfp$income<-as.factor(mhfp$income)
# mhfp$state<-as.factor(mhfp$state)
# mhfp$metro<-as.factor(mhfp$metro)
# mhfp$smoker<-as.factor(mhfp$smoker)
# mhfp$employ<-as.factor(mhfp$employ)
# mhfp$trans<-as.factor(mhfp$trans)
# mhfp$actlimit<-as.factor(mhfp$actlimit)

##Complete Case Model##
fit.cc<-glm(formula=mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=mhfp)
summval<-summary(fit.cc)$coef[,c("Estimate","Std. Error")]
summval<-summval[-c(2:19,22:28,34:37), ]
print(xtable(summary(fit.cc)$coef[,c("Estimate","Std. Error")]))

#str(imputedat$imp$sexorient)
# completedatfit = complete(imputedat, action="long") 
# completedatfit$income<-as.factor(completedatfit$income)
# completedatfit$state<-as.factor(completedatfit$state)
# completedatfit$metro<-as.factor(completedatfit$metro)
# completedatfit$smoker<-as.factor(completedatfit$smoker)
# completedatfit$employ<-as.factor(completedatfit$employ)
# completedatfit$trans<-as.factor(completedatfit$trans)
# completedatfit$actlimit<-as.factor(completedatfit$actlimit)

##Pooled Imputed Model##
mi.fit=glm.mids(mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=imputedat)
summary(mi.fit)
comb.test = pool(mi.fit)
##Summary Table##
print(xtable(summary(comb.test)[,c("est","se","fmi")]))
```


```{r}
library(mice)
library(VIM)
library(xtable)
load("imputedsets50.Rda")
load("completeimputed50.Rda")
str(imputedat$imp$sexorient)
completedatfit50 = complete(imputedat50, action="long") 

completedatfit50$income<-as.factor(completedatfit50$income)
completedatfit50$state<-as.factor(completedatfit50$state)
completedatfit50$metro<-as.factor(completedatfit50$metro)
completedatfit50$smoker<-as.factor(completedatfit50$smoker)
completedatfit50$employ<-as.factor(completedatfit50$employ)
completedatfit50$trans<-as.factor(completedatfit50$trans)
completedatfit50$actlimit<-as.factor(completedatfit50$actlimit)

head(completedatfit50)
##Pooled Imputed Model##
mi.fit50=glm.mids(mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=imputedat50)
head(summary(mi.fit50))
comb.test50 = pool(mi.fit50)
##Summary Table##
print(xtable(summary(comb.test50)[,c("est","se","fmi")]))
```


##Validity Study##
```{r}
require(mosaic)
set.seed(500)
val<-mhfp[complete.cases(mhfp),]
val2<-val[sample(nrow(val), 20876),]

set.seed(500)
random<-sample(0:1, size=20876, replace=TRUE)
randodat<-val2
randodat<-mutate(randodat, mental=ifelse(random==1, NA, mental))
randodat<-mutate(randodat, metro=ifelse(random==1, NA, metro))
randodat<-mutate(randodat, sleep=ifelse(random==1, NA, sleep))

# library(mice)
# library(VIM)
# md.pattern(randodat)
# miplot2<-aggr(randodat, col=c('red','blue'),
#                     numbers=TRUE, sortVars=TRUE,
#                     labels=names(mhfp), cex.axis=.7,
#                     gap=3, ylab=c("Missing data","Pattern"))
# 
# ##20 iterations##
# imputedatval<-mice(randodat, m=10, maxit = 20, method = 'sample', seed = 500, print=TRUE)
# summary(imputedatval)
# compdatval<-complete(imputedatval,2)
# save(imputedatval, file = "imputedsetsval.Rda")
# save(compdatval, file = "completeimputedval.Rda")
# 
# ##50 iterations##
# imputedat50val<-mice(randodat, m=10, maxit = 50, method = 'sample', seed = 500, print=TRUE)
# summary(imputedat50val)
# compdat50val<-complete(imputedat50val,2)
# save(imputedat50val, file = "imputedsets50val.Rda")
# save(compdat50val, file="completeimputed50val.Rda")
```


```{r}  
library(mice)
library(VIM)
library(xtable)
load("imputedsetsval.Rda")
load("completeimputedval.Rda")
##Truth Model##
fit.tval<-glm(formula=mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=val2)
summtruth<-summary(fit.tval)$coef[,c("Estimate","Std. Error")]
summtruth<-summtruth[-c(2:19,22:28,34:37), ]
print(xtable(summtruth))

##Complete Case Model##
fit.ccval<-glm(formula=mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=randodat)
summval<-summary(fit.cc)$coef[,c("Estimate","Std. Error")]
summval<-summval[-c(2:19,22:28,34:37), ]
print(xtable(summary(fit.cc)$coef[,c("Estimate","Std. Error")]))

#str(imputedat$imp$sexorient)
completedatfitval = complete(imputedatval, action="long") 

completedatfitval$income<-as.factor(completedatfitval$income)
completedatfitval$state<-as.factor(completedatfitval$state)
completedatfitval$metro<-as.factor(completedatfitval$metro)
completedatfitval$smoker<-as.factor(completedatfitval$smoker)
completedatfitval$employ<-as.factor(completedatfitval$employ)
completedatfitval$trans<-as.factor(completedatfitval$trans)
completedatfitval$actlimit<-as.factor(completedatfitval$actlimit)

head(completedatfitval)
##Pooled Imputed Model##
mi.fitval=glm.mids(mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=imputedatval)
head(summary(mi.fitval))
comb.testval = pool(mi.fitval)
##Summary Table##
print(xtable(summary(comb.testval)[,c("est","se","fmi")]))
```

```{r}
##Code used from Ken Kleinmann lectures##
library(mice)
library(VIM)
library(xtable)
load("imputedsets50val.Rda")
load("completeimputed50val.Rda")
str(imputedat50val$imp$sexorient)
completedatfit50val = complete(imputedat50val, action="long") 

completedatfit50val$income<-as.factor(completedatfit50val$income)
completedatfit50val$state<-as.factor(completedatfit50val$state)
completedatfit50val$metro<-as.factor(completedatfit50val$metro)
completedatfit50val$smoker<-as.factor(completedatfit50val$smoker)
completedatfit50val$employ<-as.factor(completedatfit50val$employ)
completedatfit50val$trans<-as.factor(completedatfit50val$trans)
completedatfit50val$actlimit<-as.factor(completedatfit50val$actlimit)

head(completedatfit50val)
##Pooled Imputed Model##
mi.fit50val=glm.mids(mental~as.factor(state) + sleep + male + as.factor(employ) + actlimit + as.factor(metro) + exer30 + as.factor(race) + as.factor(income) + as.factor(smoker) + binge, family="binomial", data=imputedat50val)
head(summary(mi.fit50val))
comb.test50val = pool(mi.fit50val)
##Summary Table##
print(xtable(summary(comb.test50val)[,c("est","se","fmi")]))
```


\pagebreak

#Automatic Model Selection Methods  

##Introduction  

The goal of my project is to test the performance of the different model selection algorithms of ridge, LASSO, and stepwise regression. LASSO and Ridge regressions are both forms of coefficient regularization. In addition to minimizing the residual sums of squares, these methods penalize covariates in our model based on some constraints. For ridge regression this can be written as: $\sum_{j=1}^p \beta^2_j < c$ with p = number of predictors and c a constant. For LASSO, this can be written as $\sum_{j=1}^p |\beta_j| < c$ [@stanford]. The important distinction between these two methods is that the LASSO method acutally drops predictors out of the model while the ridge method only shrinks coefficients close to 0.   

Stepwise regression begins with a set of candidate predictors and adds or removes variables based on improvements to the model's AIC. Forward selection begins with no predictors and tests which one additional variable will improve the model. This continues until AIC improvements stop. Backwards selection begins with all available predictors and removes them one-by-one until AIC improvements stop [@ncss]. For the purpose of this study, we will focus on a combination of both forward and backward selection. This can be done usind the the `both` option in the      

```{r, include=FALSE, message=FALSE}
require(glmnet)
require(MASS)
require(pROC)
require(mosaic)
require(plotmo)
require(xtable)
options(xtable.comment = FALSE)
```


##Step 1: Performance Assessment Based on Observed Data   

###Specifications  

To begin, I fit these three algorithms on the full data.   

```{r, echo=FALSE, fig.height=4}
load("coco.Rda")

#par(mfrow = c(1, 2))
#plot(ridge, xvar = "dev", xlab = "Ridge Regression")
#plot(lasso, xvar = "dev", xlab = "LASSO Regression")
par(mfrow = c(1, 2))
plot_glmnet(ridge, xvar = "dev", xlab = "Ridge Regression")
plot_glmnet(lasso, xvar = "dev", xlab = "LASSO Regression")
#title("Coefficient Shrinkage vs. Explained Deviance", outer=TRUE)
```   

The plot above show the coefficient shrinkage as a function of the model's fraction of explained deviance. Each curve represents a predictor. At the far right, all predictors are unpenalized. As you move to the left, the explained deviance decreases as the coefficients shrink closer to 0. The numbers along the upper x-axis represent the number of predictors remaining in the model. The Ridge model maintains all 14 variables even when the explained deviance is essentially zero. At this point in the LASSO plot, 0 predictors remain. The Ridge method shrinks coefficients very _close_ to 0, but the LASSO method drops some coefficients exactly to 0, removing them completely from the model. 

```{r, echo=FALSE, results ='asis'}
step1 <- filter(coefficients, variable %in% c("binge", "bmi", "exer30", "male", "sleep"))
print(xtable(step1, digits = 4), caption = "Subset of Coefficient Estimates from Selection Algorithms",
      caption.placement = "top")
```  
As shown above, most of the coefficient estimates are similar between the algorithms. However, for `bmi` the stepwise and ridge methods estimate oppisite directions of association with the probability of having poor mental health, while the LASSO method drops it out of the model completely. However, across the three methods, it does not seem to have a large effect on mental health. `Exer30` has consistent estimates through the algorithms. Each estimate a coefficient of about -0.10 with $e^{-0.10} = 0.90$. This can be interpreted to mean that those who have exercised within the past 30 days have a 10% lower odds of having experienced poor mental health.  

Again the ridge model keeps all 14 variables. The stepwise models also maintains all of the variables except for `metro` and `trans`. The LASSO menthod keeps only 10, removing `bmi`, `actlimit`, `race`, `state`, and `trans`.   

###Performance  

The previous analyses were trained on the entire data. To test each algorithm's performace, a 10-fold cross validation is run. This method divides the data into 10 partitions. For each partition, the mehtods are trained on 90% of the data and tested on the remained 10%. For each model and each iteration, a receiver operating characteristic curve is created. The average area under the curve for model is display above. We use this metric to as our measure of performance. The results are displayed below.  

```{r, echo = FALSE, results='asis'}
compare <- data.frame(Mean.AUC = round(c(mean(auc.test.aic), mean(auc.test.lasso), 
                                         mean(auc.test.ridge)), 3), 
                      SD.AUC = c(round(sd(auc.test.aic), 5), round(sd(auc.test.lasso), 5),
                                 round(sd(auc.test.ridge),5)))
rownames(compare) <- c("Stepwise", "LASSO", "Ridge")
print(xtable(compare, digits = 5), caption = "Mean Areas Under The Curve from Cross Validation",
      caption.placement = 'top')
```  
As can be seen above, the stepwise method yields the highest area under the curve across the 10 iterations. This model will be used in subsequent analyses.  

##Step 2: Simulation Study    

The next step in this investigation is running a simulation study. A simulation study is a computer-based experiment involving randomly generated samples of the data [@sim]. We will do this by sampling $n$ times from the original data. We will then generate new mental health values using the model specified by the stepwise selection algorithm outlined above. This means that in this case, we will know the true outcome generating process. This model icludes factors levels for every state, employment, and income level, thus an abridged version is written below:  

$$logit(mental) = 1.10 -0.29(actlimit = 1) + 0.42(binge) + 0.02(bmi) - 0.21(employ = 2) + 0.38(employ = 3)$$  
$$ - 0.50(male) - 0.11(sleep) + 0.21(trans =1)$$  

In order to test how the performance of these algorithms depend on sample size, we reran them on samples of size 10,000 and 100,000. The means and standard deviations across these iterations are displayed below.    

__Note:__ These datasets include all 14 variables originally available for the models to choose from. This means that this includes `metro` and `trans` which were not variables used to generate outcomes from the stepwise model.


```{r echo = FALSE, results='asis'}
compare.sim <- data.frame(Method = c("LASSO", "Ridge", "Stepwise"),
                          `Mean 10,000` = round(c(mean(auc10_lasso), mean(auc10_ridge), mean(auc10_aic)), 4),
                          `SD 10,000` = round(c(sd(auc10_lasso), sd(auc10_ridge), sd(auc10_aic)), 5),
                          `Mean 100,000` = round(c(mean(auc1000_lasso), mean(auc1000_ridge), mean(auc1000_aic)), 4),
                          `SD 100,000` = round(c(sd(auc1000_lasso), sd(auc1000_ridge), sd(auc1000_aic)), 5))
#xtable(compare.sim)
print(xtable(compare.sim, digits = 5), caption = "Performance Metrics for Simulation Study", caption.placement = 'top')
```




##Conclusions  

Our analyses have consistently shown stepwise regression to have the highest performance in terms of prediction for this dataset. It steadily resulted in the highest area under the curve.   



Stepwise selection has been criticized since the stronger computing power has enabled us to use more computationally intensive methods such as LASSO selection. Stepwise's main advantages are that it is easy to implement and easy to understand[@stack]. However, it is associated with a lot of disadvantages as well. Firstly, the stepwise approach is highly dependent on its initial set of candidate predictors. It is known to fall victim to finding spurious assocations [@stack3]. Because it makes decisions at every step, it can make choices that are "locally optimal, but suboptimal in general" [@stack2]. These arbitrary choices can cause "severe biases in the resulting multivariable model fits while losing valuable predictive information from deleting marginally significant variables" [@stack]. Thus overall, this method can result in strong biases and deceptive findings.

So why has the stepwise algorithm outperformed the others with this data? This could perhaps be because although there is a large sample size, there are only 14 predictors to parse through. LASSO regression typically does better than stepwise and ridge regression methods when the number of predictors, p > n. This is not the case in this study. Often a concern for stepwise regression is overfitting which is also likely to occur when p > n. Here though, this is not a worry. However, it is not completely surprising that in this context, the stepwise selection yields different results that the others becuase its algorithm is based on AIC rather than the penalization restraints used by LASSO and ridge regressions.  

Overall, although this study found the stepwise algorithm to have the best performance, I would not say this method is the best selection by any means. Just because its model performed well here, it does not mean it will in other contexts. One huge disadvantage to stepwise is the computing time necessary with large sample sizes. For samples sized below 1,000 and 10 iterations, it fit in just under a minute. However, with sizes over 5,000, the time proliferated. The simulations with i = 100 and samples of 100,000 observations, the stepwise model took over 12 hours to run. Thus for larger samples and more iterations stepwise selection would be relatively infeasible in comparison to LASSO and rigde regression which took less than 10% of the time. It seems that each method may have its place in different situations. For example, ridge outperforms both LASSO and stepwise when the effects are better predicted by a combination of many weak predictors [@stack2]. In conclusion, although stepwise regression performs well with these data, it is not the best model selection algorithm across the board and multiple methods should always be implemented and compared whenever it is practical to do so.   


\newpage

#Random Effect Models 



# References
